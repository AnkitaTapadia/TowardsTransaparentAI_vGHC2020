{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Sentiment Analysis and the Unified Information Explainer Light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`**. This will run the notebook on a small subset of the data and a use a smaller number of epochs. \n",
    "\n",
    "If you run into CUDA out-of-memory error or the jupyter kernel dies constantly, try reducing the `BATCH_SIZE` and `MAX_LEN`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nlp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from interpret_text.experimental.common.utils_bert import Language, Tokenizer, BERTSequenceClassifier\n",
    "from interpret_text.experimental.unified_information import UnifiedInformationExplainer\n",
    "from interpret_text.experimental.common.timer import Timer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [BERT](https://arxiv.org/abs/1810.04805) model on a subset of the [Emo](https://huggingface.co/datasets/emo/) dataset.\n",
    "\n",
    "We use a [sequence classifier](https://github.com/microsoft/nlp/blob/master/utils_nlp/models/bert/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of Google's [BERT](https://github.com/google-research/bert).\n",
    "\n",
    "We then show how to use the [interpret-text](https://github.com/interpretml/interpret-text) package to explain the outcome of the model. More example notebooks and NLP explainer models can be found on the [github page](https://github.com/interpretml/interpret-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "First we set some parameters that we use for our modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_FRACTION = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_FRACTION = 0.01\n",
    "    TEST_DATA_FRACTION = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 1\n",
    "else:\n",
    "    BATCH_SIZE = 8\n",
    "\n",
    "DATA_FOLDER = \"./temp\"\n",
    "BERT_CACHE_DIR = \"./temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE_PRED = 512\n",
    "TRAIN_SIZE = 0.6\n",
    "EMOTION_COL = \"emotion\"\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We then load the data.\n",
    "\n",
    "In this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others.\n",
    "\n",
    "The Emo dataset is mainly used for sentiment analysis task, where the input is a textual dialogue and the labels are underlying emtion of the dialogue. Given the input, the goal is to identify this underlying emotion from four classes - *happy, sad, angry and others.*\n",
    "\n",
    "For our classification task, we use three of the classes - happy, sad, angry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = nlp.load_dataset(\"emo\", split = [\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'others', 1: 'happy', 2: 'sad', 3: 'angry'}\n",
    "labels=list(id2label.values())\n",
    "label2id = {}\n",
    "for i,label in enumerate(labels):\n",
    "    label2id[label]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data={TEXT_COL:[],\n",
    "     EMOTION_COL:[]}\n",
    "for val in train:\n",
    "    if id2label[val[LABEL_COL]]!='others':\n",
    "        train_data[TEXT_COL].append(val[TEXT_COL])\n",
    "        train_data[EMOTION_COL].append(id2label[val[LABEL_COL]])\n",
    "        \n",
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data={TEXT_COL:[],\n",
    "     EMOTION_COL:[]}\n",
    "for val in test:\n",
    "    if id2label[val[LABEL_COL]]!='others':\n",
    "        test_data[TEXT_COL].append(val[TEXT_COL])\n",
    "        test_data[EMOTION_COL].append(id2label[val[LABEL_COL]])\n",
    "        \n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if QUICK_RUN:\n",
    "    train_data = train_data.sample(frac=TRAIN_DATA_FRACTION).reset_index(drop=True)\n",
    "    test_data = test_data.sample(frac=TEST_DATA_FRACTION).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[LABEL_COL] = label_encoder.fit_transform(train_data[EMOTION_COL])\n",
    "test_data[LABEL_COL] = label_encoder.transform(test_data[EMOTION_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has three emotions. Here is the distribution in the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = train_data.emotion.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = test_data.emotion.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sad      59\n",
       "angry    57\n",
       "happy    36\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[EMOTION_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sad      5\n",
       "angry    2\n",
       "happy    1\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[EMOTION_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 152\n",
      "Number of testing examples: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: {}\".format(train_data.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a `BERT tokenizer` given the language, and tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")# if not torch.cuda.is_available() else \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 152/152 [00:00<00:00, 4170.92it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 2668.34it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "\n",
    "tokens_train = tokenizer.tokenize(list(train_data[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(test_data[TEXT_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence, respectively\n",
    "- Pad or truncate the token lists to the specified max length. In this case, `MAX_LEN = 150`\n",
    "- Return mask lists that indicate the paddings' positions\n",
    "- Return token type id lists that indicate which sentence the tokens belong to (not needed for one-sequence classification)\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(tokens_train, MAX_LEN)\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(tokens_test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classifier Model\n",
    "Next, we use a sequence classifier that loads a pre-trained BERT model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(language=LANGUAGE, num_labels=3, cache_dir=BERT_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We train the classifier using the training set. This involves fine-tuning the BERT Transformer and learning a linear classification layer on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Iteration:   0%|                                                                                | 0/19 [00:00<?, ?it/s]c:\\users\\jamahaja\\miniconda3\\envs\\myenv\\lib\\site-packages\\pytorch_pretrained_bert\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Iteration: 100%|███████████████████████████████████████████████████████████████████████| 19/19 [02:19<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training time: 0.039 hrs]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(token_ids=tokens_train,\n",
    "                    input_mask=mask_train,\n",
    "                    labels=train_data[LABEL_COL],    \n",
    "                    num_epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,    \n",
    "                    verbose=True)\n",
    "   \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|█████████████████████████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.75s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = classifier.predict(token_ids=tokens_test, \n",
    "                           input_mask=mask_test, \n",
    "                           batch_size=BATCH_SIZE_PRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Finally, we compute the overall accuracy, precision, recall, and F1 metrics on the test set. We also look at the metrics for eact of the genres in the the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5\n",
      "{\n",
      "    \"accuracy\": 0.5,\n",
      "    \"angry\": {\n",
      "        \"f1-score\": 0.0,\n",
      "        \"precision\": 0.0,\n",
      "        \"recall\": 0.0,\n",
      "        \"support\": 2\n",
      "    },\n",
      "    \"happy\": {\n",
      "        \"f1-score\": 0.0,\n",
      "        \"precision\": 0.0,\n",
      "        \"recall\": 0.0,\n",
      "        \"support\": 1\n",
      "    },\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.2222222222222222,\n",
      "        \"precision\": 0.19047619047619047,\n",
      "        \"recall\": 0.26666666666666666,\n",
      "        \"support\": 8\n",
      "    },\n",
      "    \"sad\": {\n",
      "        \"f1-score\": 0.6666666666666666,\n",
      "        \"precision\": 0.5714285714285714,\n",
      "        \"recall\": 0.8,\n",
      "        \"support\": 5\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.41666666666666663,\n",
      "        \"precision\": 0.3571428571428571,\n",
      "        \"recall\": 0.5,\n",
      "        \"support\": 8\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\jamahaja\\miniconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_data[LABEL_COL], preds, target_names=label_encoder.classes_, output_dict=True) \n",
    "accuracy = accuracy_score(test_data[LABEL_COL], preds)\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "print(json.dumps(report, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")# if not torch.cuda.is_available() else \"cuda\")\n",
    "#download the trained model\n",
    "classifier.model.to(device)\n",
    "for param in classifier.model.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_unified = UnifiedInformationExplainer(model=classifier.model, \n",
    "                                 train_dataset=list(train_data[TEXT_COL]), \n",
    "                                 device=device, \n",
    "                                 target_layer=14,\n",
    "                                classes=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am always single i'm in a happy relationship and i agree with you know one love mein don't reason ; true label: ['sad'] ; predicted label: ['sad']\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "text = test_data[TEXT_COL][idx]\n",
    "true_label = label_encoder.inverse_transform([test_data[LABEL_COL][idx]])\n",
    "predicted_label = label_encoder.inverse_transform([preds[idx]])\n",
    "print(text, '; true label:', true_label,'; predicted label:', predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 152/152 [00:00<00:00, 4126.25it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [02:17<00:00,  1.09it/s]\n"
     ]
    }
   ],
   "source": [
    "explanation_unified = interpreter_unified.explain_local(text, 'happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_text.experimental.widget import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ed19e5c1424a82b8b56957cf3bc44e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExplanationWidget(value={'text': ['i', 'am', 'always', 'single', 'i', \"'\", 'm', 'in', 'a', 'happy', 'relations…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<interpret_text.experimental.widget.ExplanationDashboard.ExplanationDashboard at 0x1f72adfdcf8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExplanationDashboard(explanation_unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
