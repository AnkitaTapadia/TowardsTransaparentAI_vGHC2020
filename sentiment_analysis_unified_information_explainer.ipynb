{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Copyright (c) Microsoft Corporation. All rights reserved.*\n",
    "\n",
    "*Licensed under the MIT License.*\n",
    "\n",
    "# Sentiment Analysis and the Unified Information Explainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before You Start\n",
    "\n",
    "> **Tip**: If you want to run through the notebook quickly, you can set the **`QUICK_RUN`** flag in the cell below to **`True`**. This will run the notebook on a small subset of the data and a use a smaller number of epochs. \n",
    "\n",
    "If you run into CUDA out-of-memory error or the jupyter kernel dies constantly, try reducing the `BATCH_SIZE` and `MAX_LEN`, but note that model performance will be compromised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set QUICK_RUN = True to run the notebook on a small subset of data and a smaller number of epochs.\n",
    "QUICK_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import nlp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from interpret_text.experimental.common.utils_bert import Language, Tokenizer, BERTSequenceClassifier\n",
    "from interpret_text.experimental.unified_information import UnifiedInformationExplainer\n",
    "from interpret_text.experimental.common.timer import Timer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, we fine-tune and evaluate a pretrained [BERT](https://arxiv.org/abs/1810.04805) model on a subset of the [Emo](https://huggingface.co/datasets/emo/) dataset.\n",
    "\n",
    "We use a [sequence classifier](https://github.com/microsoft/nlp/blob/master/utils_nlp/models/bert/sequence_classification.py) that wraps [Hugging Face's PyTorch implementation](https://github.com/huggingface/pytorch-pretrained-BERT) of Google's [BERT](https://github.com/google-research/bert).\n",
    "\n",
    "We then show how to use the [interpret-text](https://github.com/interpretml/interpret-text) package to explain the outcome of the model. More example notebooks and NLP explainer models can be found on the [github page](https://github.com/interpretml/interpret-text)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "First we set some parameters that we use for our modeling task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATA_FRACTION = 1\n",
    "NUM_EPOCHS = 1\n",
    "\n",
    "if QUICK_RUN:\n",
    "    TRAIN_DATA_FRACTION = 0.01\n",
    "    TEST_DATA_FRACTION = 0.01\n",
    "    NUM_EPOCHS = 1\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    BATCH_SIZE = 1\n",
    "else:\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "DATA_FOLDER = \"./temp\"\n",
    "BERT_CACHE_DIR = \"./temp\"\n",
    "LANGUAGE = Language.ENGLISH\n",
    "TO_LOWER = True\n",
    "MAX_LEN = 150\n",
    "BATCH_SIZE_PRED = 2\n",
    "TRAIN_SIZE = 0.6\n",
    "EMOTION_COL = \"emotion\"\n",
    "LABEL_COL = \"label\"\n",
    "TEXT_COL = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Dataset\n",
    "We then load the data.\n",
    "\n",
    "In this dataset, given a textual dialogue i.e. an utterance along with two previous turns of context, the goal was to infer the underlying emotion of the utterance by choosing from four emotion classes - Happy, Sad, Angry and Others.\n",
    "\n",
    "The Emo dataset is mainly used for sentiment analysis task, where the input is a textual dialogue and the labels are underlying emtion of the dialogue. Given the input, the goal is to identify this underlying emotion from four classes - *happy, sad, angry and others.*\n",
    "\n",
    "For our classification task, we use three of the classes - happy, sad, angry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, test = nlp.load_dataset(\"emo\", split = [\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'others', 1: 'happy', 2: 'sad', 3: 'angry'}\n",
    "labels=list(id2label.values())\n",
    "label2id = {}\n",
    "for i,label in enumerate(labels):\n",
    "    label2id[label]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data={TEXT_COL:[],\n",
    "     EMOTION_COL:[]}\n",
    "for val in train:\n",
    "    if id2label[val[LABEL_COL]]!='others':\n",
    "        train_data[TEXT_COL].append(val[TEXT_COL])\n",
    "        train_data[EMOTION_COL].append(id2label[val[LABEL_COL]])\n",
    "        \n",
    "train_data = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data={TEXT_COL:[],\n",
    "     EMOTION_COL:[]}\n",
    "for val in test:\n",
    "    if id2label[val[LABEL_COL]]!='others':\n",
    "        test_data[TEXT_COL].append(val[TEXT_COL])\n",
    "        test_data[EMOTION_COL].append(id2label[val[LABEL_COL]])\n",
    "        \n",
    "test_data = pd.DataFrame(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_data[LABEL_COL] = label_encoder.fit_transform(train_data[EMOTION_COL])\n",
    "test_data[LABEL_COL] = label_encoder.transform(test_data[EMOTION_COL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Emotion labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has three emotions. Here is the distribution in the training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = train_data.emotion.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = test_data.emotion.value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry    5506\n",
       "sad      5463\n",
       "happy    4243\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[EMOTION_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "angry    298\n",
       "happy    284\n",
       "sad      250\n",
       "Name: emotion, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[EMOTION_COL].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 15212\n",
      "Number of testing examples: 832\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples: {}\".format(train_data.shape[0]))\n",
    "print(\"Number of testing examples: {}\".format(test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start training, we tokenize the text documents and convert them to lists of tokens. The following steps instantiate a `BERT tokenizer` given the language, and tokenize the text of the training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")# if not torch.cuda.is_available() else \"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 15212/15212 [00:13<00:00, 1135.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 832/832 [00:00<00:00, 1201.42it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(LANGUAGE, to_lower=TO_LOWER, cache_dir=BERT_CACHE_DIR)\n",
    "\n",
    "tokens_train = tokenizer.tokenize(list(train_data[TEXT_COL]))\n",
    "tokens_test = tokenizer.tokenize(list(test_data[TEXT_COL]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we perform the following preprocessing steps in the cell below:\n",
    "- Convert the tokens into token indices corresponding to the BERT tokenizer's vocabulary\n",
    "- Add the special tokens [CLS] and [SEP] to mark the beginning and end of a sentence, respectively\n",
    "- Pad or truncate the token lists to the specified max length. In this case, `MAX_LEN = 150`\n",
    "- Return mask lists that indicate the paddings' positions\n",
    "- Return token type id lists that indicate which sentence the tokens belong to (not needed for one-sequence classification)\n",
    "\n",
    "*See the original [implementation](https://github.com/google-research/bert/blob/master/run_classifier.py) for more information on BERT's input format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train, mask_train, _ = tokenizer.preprocess_classification_tokens(tokens_train, MAX_LEN)\n",
    "tokens_test, mask_test, _ = tokenizer.preprocess_classification_tokens(tokens_test, MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Classifier Model\n",
    "Next, we use a sequence classifier that loads a pre-trained BERT model, given the language and number of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = BERTSequenceClassifier(language=LANGUAGE, num_labels=3, cache_dir=BERT_CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "We train the classifier using the training set. This involves fine-tuning the BERT Transformer and learning a linear classification layer on top of that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n",
      "Iteration:   0%|                                                                              | 0/7606 [00:00<?, ?it/s]c:\\users\\jamahaja\\miniconda3\\envs\\myenv\\lib\\site-packages\\pytorch_pretrained_bert\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Iteration: 100%|█████████████████████████████████████████████████████████████████| 7606/7606 [6:18:38<00:00,  2.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Training time: 6.311 hrs]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with Timer() as t:\n",
    "    classifier.fit(token_ids=tokens_train,\n",
    "                    input_mask=mask_train,\n",
    "                    labels=train_data[LABEL_COL],    \n",
    "                    num_epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,    \n",
    "                    verbose=True)\n",
    "   \n",
    "print(\"[Training time: {:.3f} hrs]\".format(t.interval / 3600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Model\n",
    "We score the test set using the trained classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|████████████████████████████████████████████████████████████████████████| 2/2 [03:35<00:00, 107.79s/it]\n"
     ]
    }
   ],
   "source": [
    "preds = classifier.predict(token_ids=tokens_test, \n",
    "                           input_mask=mask_test, \n",
    "                           batch_size=BATCH_SIZE_PRED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Finally, we compute the overall accuracy, precision, recall, and F1 metrics on the test set. We also look at the metrics for eact of the genres in the the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.9170673076923077\n",
      "{\n",
      "    \"accuracy\": 0.9170673076923077,\n",
      "    \"angry\": {\n",
      "        \"f1-score\": 0.9218241042345278,\n",
      "        \"precision\": 0.8955696202531646,\n",
      "        \"recall\": 0.9496644295302014,\n",
      "        \"support\": 298\n",
      "    },\n",
      "    \"happy\": {\n",
      "        \"f1-score\": 0.9355932203389831,\n",
      "        \"precision\": 0.9019607843137255,\n",
      "        \"recall\": 0.971830985915493,\n",
      "        \"support\": 284\n",
      "    },\n",
      "    \"macro avg\": {\n",
      "        \"f1-score\": 0.9147912821042138,\n",
      "        \"precision\": 0.9229863253318205,\n",
      "        \"recall\": 0.9124984718152315,\n",
      "        \"support\": 832\n",
      "    },\n",
      "    \"sad\": {\n",
      "        \"f1-score\": 0.8869565217391304,\n",
      "        \"precision\": 0.9714285714285714,\n",
      "        \"recall\": 0.816,\n",
      "        \"support\": 250\n",
      "    },\n",
      "    \"weighted avg\": {\n",
      "        \"f1-score\": 0.9160471010492105,\n",
      "        \"precision\": 0.9205453755260625,\n",
      "        \"recall\": 0.9170673076923077,\n",
      "        \"support\": 832\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(test_data[LABEL_COL], preds, target_names=label_encoder.classes_, output_dict=True) \n",
    "accuracy = accuracy_score(test_data[LABEL_COL], preds)\n",
    "print(\"accuracy: {}\".format(accuracy))\n",
    "print(json.dumps(report, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): BertLayerNorm()\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): BertLayerNorm()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")# if not torch.cuda.is_available() else \"cuda\")\n",
    "#download the trained model\n",
    "classifier.model.to(device)\n",
    "for param in classifier.model.parameters():\n",
    "    param.requires_grad = False\n",
    "classifier.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter_unified = UnifiedInformationExplainer(model=classifier.model, \n",
    "                                 train_dataset=list(train_data[TEXT_COL]), \n",
    "                                 device=device, \n",
    "                                 target_layer=14,\n",
    "                                classes=label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is my grammar perfect or should i need to learn more yes it is possible thank you ; true label: ['happy'] ; predicted label: ['happy']\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "text = test_data[TEXT_COL][idx]\n",
    "true_label = label_encoder.inverse_transform([test_data[LABEL_COL][idx]])\n",
    "predicted_label = label_encoder.inverse_transform([preds[idx]])\n",
    "print(text, '; true label:', true_label,'; predicted label:', predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 4537.95it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 150/150 [02:12<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "explanation_unified = interpreter_unified.explain_local(text, 'happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret_text.experimental.widget import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b3d75593c89486eba83ec19206184b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExplanationWidget(value={'text': ['is', 'my', 'grammar', 'perfect', 'or', 'should', 'i', 'need', 'to', 'learn'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<interpret_text.experimental.widget.ExplanationDashboard.ExplanationDashboard at 0x1e124e715f8>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ExplanationDashboard(explanation_unified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
